# Line search project

## Datasets
- âœ” get ZINC dataset working
## GNN
- âœ” implement basic gnn
- âœ” incorporate line search into gnn
- âœ” update to proper to newest algorithm from phillips email
- âœ” try different c values 
- âœ” compares SalSA mit AdamSLS
  - âœ” fix how SalSA data is saved
  - âœ” Include in bar charts
- finalize traning script
- mv parameter (without first order momentum = AdamSLS?)

## Results
- train everything on ~100 epochs
- âœ” whats up with test loss? Try proper shuffle
- ðŸ¤” test data makes sense? -> as far as i can tell yes it is just worse than default Adam
- table verglieich final val/c/optim/data etc.
  - What does that even mean? :D
  - Hab jz nen bar chart gemacht? vll table im report!
### Graphs
visualize:
- âœ” train/test loss
- âœ” learning rate adam vs adaptive (get from optimizer.state during runtime!)
- âœ” viz for different c values
- âœ” make log scale -> doesnt work
- âœ” include c value in legend
- âœ” consistant colors when selecting options
- âœ” Rangeslider
- âœ” ein graph pro val/train nicht zsm
  - Or grey out deselected lines maybe 
  - âœ”  one graph compares "best c value" and the other one compares different c values?
  - âœ” But seperate train/test probably makes sense

### Report
- start writing report with chapters of what i did and why
### Notes

### future
- loss decrease SaLSA.py?
- interesannte sachen z.b. gradient norm in state packen
- sufficent decerase tracken, warum negativ?
- compare to Adam with lr scheduler
# def train(model, optimizer, features, adjacency_matrix, target):
    model.train()
    optimizer.zero_grad()

    # Encapsulate forward pass within a closure function
    closure = lambda: torch.mean((model(features, adjacency_matrix) - target) ** 2)

    # Use the optimizer's closure mechanism
    loss = optimizer.step(closure=closure)
    return loss.item()